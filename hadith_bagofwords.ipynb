{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ufal.udpipe import Model, Pipeline, ProcessingError\n",
    "import pandas as pd \n",
    "from conllu import parse\n",
    "\n",
    "model = Model.load('G:/Softwares/urdu-udtb-ud-2.3-181115.udpipe')\n",
    "pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "error = ProcessingError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To get row count only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\envs\\islamic qa\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6709\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "conn = pyodbc.connect(r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=G:\\Softwares\\EQWH-V6.4-190407\\EasyHadees - Copy.mdb;')\n",
    "cursor = conn.cursor()\n",
    "targetcount = 100\n",
    "cursor.execute(\"select count(*) from ahadith where urdutext is not null and urdutext like '%وضو%'\")\n",
    "\n",
    "count = cursor.fetchone()[0]\n",
    "print(count)\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "start = time.time()\n",
    "\n",
    "conn = pyodbc.connect(r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=G:\\Softwares\\EQWH-V6.4-190407\\EasyHadees - Copy.mdb;')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"select top 5 id, urdutext from ahadith where urdutext is not null and urdutext like '%وضو%'\")\n",
    "\n",
    "Y = []\n",
    "corpus = []\n",
    "lens_of_d= []\n",
    "for row in cursor.fetchall():\n",
    "    hadith = row[1].replace('صلی اللہ علیہ وآلہ وسلم','')    \n",
    "    corpus.append(hadith)\n",
    "    lens_of_d.append(len(hadith.split()))\n",
    "    Y.append(row[0])\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "avg_docs_len = np.mean(lens_of_d, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "# -*- coding: utf-8 -*-\n",
    "STOP_WORDS = \"\"\"\n",
    " ابھی از \n",
    " اس استعمال اسی اسے البتہ الف ان اندر اور اوپر اکثر اگر\n",
    " اگرچہ اگلے ایسا ایسی ایسے اے بار بارے باوجود باہر بظاہر بعض بغیر بلکہ بن\n",
    " بھر بھریں بھی بہت بے تا تاکہ تب تحت تر تمام\n",
    " تو تک جب سوال \n",
    " جبکہ جو حالانکہ حالاں خلاف\n",
    " خود سے میں چنانچہ دیر ذریعے تھا هے ایک آپ\n",
    " سا ساتھ سامنے سب تھی تھے\n",
    " سکا سکتا سکتے سی سے شان شاید صرف صورت ضرورت ضروری طرح طرف طور علاوہ عین غیر \n",
    " لہذا لیکن لیں لیے لے مجھ مجھے مزید مقابلے مل مکمل مگر \n",
    " نا نہ نہیں نیچے واقعی والا والوں والی والے وجہ وغیرہ وہ وہاں وہی وہیں وی ویسے پاس\n",
    " پایا پر پوری پھر پیچھے چونکہ چکی\n",
    " ڈالے کئے کافی کبھی کسی کم \n",
    " کوئی کچھ کہ کہا کہہ کہیں کہے کیونکہ کیے کے گئی\n",
    " گئے گا گویا گی گے ہاں ہر ہمیشہ ہو ہوئی ہوئیں ہوئے ہوا ہوتا\n",
    " ہوتی ہوتیں ہوتے ہونا ہونگے ہونی ہونے ہوں ہی ہیں ہے یا یات یعنی یقینا یہ یہاں یہی یہیں ھیں\n",
    "  حضرت ہوگی رض\n",
    " کا کو کی نے ہوتو پہلے کر\n",
    " اپنا حتى كہ ميں لئیے  \n",
    " بعد ارادہ افاقہ آخر\n",
    "\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer as vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#vectorizer = vectorizer()\n",
    "#X = vectorizer.fit_transform(corpus)\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "vec = CountVectorizer(stop_words=STOP_WORDS).fit(corpus)\n",
    "bag_of_words = vec.transform(corpus)\n",
    "vocab = vec.vocabulary_.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words = bag_of_words.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vocab]\n",
    "def term_freq(myword):\n",
    "    return ([tuple for tuple in words_freq if myword in tuple][0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq_doc(myword,doc_index):\n",
    "    word_freq_doc = [(word, bag_of_words[doc_index].sum(axis=0)[0, idx]) for word, idx in vocab]\n",
    "    if len(word_freq_doc) == 0:\n",
    "        return 0\n",
    "    term_tuple = [tuple for tuple in word_freq_doc if myword in tuple]\n",
    "    if len(term_tuple) ==0:\n",
    "        return 0\n",
    "    return (term_tuple[0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#term_freq_doc('پانی',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking (okapi BM25)\n",
    "\n",
    "![title](img/43e5c609557364f7836b6b2f4cd8ea41deb86a96.svg)\n",
    "\n",
    "where\n",
    "\n",
    "![title](img/c652b6871ce4872c8e924ff0f806bc8b06dc94ed.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def no_of_docs(q):\n",
    "    no_of_ds = len([di for di in range(0,len(Y)) if term_freq_doc(q,di) > 0])    \n",
    "    return no_of_ds        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def idf(q):\n",
    "    nq = no_of_docs(q)\n",
    "    ans = math.log10(  (len(Y)- nq + 0.5) / (nq+ 0.5)  )\n",
    "    return ans\n",
    "\n",
    "def bm25_score(qwords,doc_index):\n",
    "    score = 0;\n",
    "    for qw in qwords:\n",
    "        tfd = term_freq_doc(qw, doc_index)\n",
    "        score += idf(qw) * ( (tfd * (1.2 + 1))/ (tfd + 1.2 * (1-0.75 + 0.75 * (lens_of_d[doc_index] / avg_docs_len))))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('urduvec_140M_100K_300d.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'وضو کیسے کریں'\n",
    "question_arr = question.split()\n",
    "\n",
    "query_words = []\n",
    "query_words.append(question_arr)\n",
    "for question_word in question_arr:\n",
    "    alltop = model.most_similar(positive=question_word, topn=20)\n",
    "    for key, val in [item for item in alltop if item[1] > 0.51]:\n",
    "        query_words.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\envs\\islamic qa\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6807580442743947\n"
     ]
    }
   ],
   "source": [
    "score = bm25_score(query_words, 2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ابوالنعمان، ابوعوانہ، ابی بشر، یوسف بن ماہک، عبداللہ بن عمرو (رض) کہتے ہیں کہ ایک سفر میں نبی () ہم سے پیچھے رہ گئے، جب آپ ہمارے قریب پہنچے تو نماز میں تاخیر ہونے (کی وجہ سے) ہم (جلد جلد) وضو کر رہے تھے، اسی وجہ سے ہم اپنے پیروں پر پانی ملنے لگے (کیونکہ دھونے میں دیر ہوتی) پس آپ نے اپنی بلند آواز سے دو یا تین مرتبہ فرمایا کہ (پیروں کے) ٹخنوں کو آگ کے (عذاب) سے خرابی (ہونے والی) ہے۔ '"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
