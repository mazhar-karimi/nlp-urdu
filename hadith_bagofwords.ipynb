{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ufal.udpipe import Model, Pipeline, ProcessingError\n",
    "import pandas as pd \n",
    "from conllu import parse\n",
    "\n",
    "model = Model.load('G:/Softwares/urdu-udtb-ud-2.3-181115.udpipe')\n",
    "pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "error = ProcessingError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To get row count only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6709\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "conn = pyodbc.connect(r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=G:\\Softwares\\EQWH-V6.4-190407\\EasyHadees - Copy.mdb;')\n",
    "cursor = conn.cursor()\n",
    "targetcount = 100\n",
    "cursor.execute(\"select count(*) from ahadith where urdutext is not null and urdutext like '%وضو%'\")\n",
    "\n",
    "count = cursor.fetchone()[0]\n",
    "print(count)\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "conn = pyodbc.connect(r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=G:\\Softwares\\EQWH-V6.4-190407\\EasyHadees - Copy.mdb;')\n",
    "cursor = conn.cursor()\n",
    "targetcount = 100\n",
    "cursor.execute(\"select id, urdutext from ahadith where urdutext is not null and urdutext like '%وضو%'\")\n",
    "\n",
    "Y = []\n",
    "corpus = []\n",
    "for row in cursor.fetchall(): \n",
    "    corpus.append(row[1].replace('صلی اللہ علیہ وآلہ وسلم',''))\n",
    "    Y.append(row[0])\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "# -*- coding: utf-8 -*-\n",
    "STOP_WORDS = \"\"\"\n",
    " ابھی از \n",
    " اس استعمال اسی اسے البتہ الف ان اندر اور اوپر اکثر اگر\n",
    " اگرچہ اگلے ایسا ایسی ایسے اے بار بارے باوجود باہر بظاہر بعض بغیر بلکہ بن\n",
    " بھر بھریں بھی بہت بے تا تاکہ تب تحت تر تمام\n",
    " تو تک جب سوال \n",
    " جبکہ جو حالانکہ حالاں خلاف\n",
    " خود سے میں چنانچہ دیر ذریعے تھا هے ایک آپ\n",
    " سا ساتھ سامنے سب تھی تھے\n",
    " سکا سکتا سکتے سی سے شان شاید صرف صورت ضرورت ضروری طرح طرف طور علاوہ عین غیر \n",
    " لہذا لیکن لیں لیے لے مجھ مجھے مزید مقابلے مل مکمل مگر \n",
    " نا نہ نہیں نیچے واقعی والا والوں والی والے وجہ وغیرہ وہ وہاں وہی وہیں وی ویسے پاس\n",
    " پایا پر پوری پھر پیچھے چونکہ چکی\n",
    " ڈالے کئے کافی کبھی کسی کم \n",
    " کوئی کچھ کہ کہا کہہ کہیں کہے کیونکہ کیے کے گئی\n",
    " گئے گا گویا گی گے ہاں ہر ہمیشہ ہو ہوئی ہوئیں ہوئے ہوا ہوتا\n",
    " ہوتی ہوتیں ہوتے ہونا ہونگے ہونی ہونے ہوں ہی ہیں ہے یا یات یعنی یقینا یہ یہاں یہی یہیں ھیں\n",
    "  حضرت ہوگی رض\n",
    " کا کو کی نے ہوتو پہلے کر\n",
    " اپنا حتى كہ ميں لئیے  \n",
    " بعد ارادہ افاقہ آخر\n",
    "\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer as vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#vectorizer = vectorizer()\n",
    "#X = vectorizer.fit_transform(corpus)\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "vec = CountVectorizer(stop_words=STOP_WORDS).fit(corpus)\n",
    "bag_of_words = vec.transform(corpus)\n",
    "vocab = vec.vocabulary_.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-300581aae151>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msum_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbag_of_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwords_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mwords_freq\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "sum_words = bag_of_words.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vocab]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3950\n"
     ]
    }
   ],
   "source": [
    "print(([tuple for tuple in words_freq if 'پانی' in tuple][0])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ہاتھوں', 4), ('کیا', 4), ('دونوں', 4), ('مسح', 3), ('یحیی', 3), ('عبداللہ', 2), ('عمرو', 2), ('وضو', 2), ('پانی', 2), ('انہوں', 2)]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "sum_words = bag_of_words[900].sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "print(words_freq[:10])\n",
    "print(([tuple for tuple in words_freq if 'پانی' in tuple][0])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words = bag_of_words.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vocab]\n",
    "def term_freq(myword):\n",
    "    return ([tuple for tuple in words_freq if myword in tuple][0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3950"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freq('پانی')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq_doc(myword,doc_index):\n",
    "    word_freq_doc = [(word, bag_of_words[doc_index].sum(axis=0)[0, idx]) for word, idx in vocab]\n",
    "    return ([tuple for tuple in word_freq_doc if myword in tuple][0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freq_doc('پانی',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
